#!/usr/bin/env python3
"""
DeepWiki Scraper - å°† DeepWiki é¡¹ç›®æ–‡æ¡£æŠ“å–å¹¶åˆå¹¶ä¸ºå•ä¸ª Markdown æ–‡ä»¶
æ”¯æŒå¤šå¯†é’¥è½®è¯¢å’Œæ™ºèƒ½é™é€Ÿ
"""

import os
import sys
import argparse
import requests
import time
import threading
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

FIRECRAWL_API_URL = 'https://api.firecrawl.dev/v1/scrape'
DEEPWIKI_BASE = 'https://deepwiki.com'


class APIKeyManager:
    """API å¯†é’¥ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šå¯†é’¥è½®è¯¢"""
    
    def __init__(self):
        self.keys = self._load_keys()
        self.current_index = 0
        self.lock = threading.Lock()
        
    def _load_keys(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½æ‰€æœ‰å¯ç”¨çš„ API å¯†é’¥"""
        keys = []
        
        # ä¸»å¯†é’¥
        main_key = os.getenv('FIRECRAWL_API_KEY')
        if main_key:
            keys.append(main_key)
        
        # å…¶ä»–å‘½åå¯†é’¥
        key_suffixes = ['LASALLE', 'FALLY', 'JIASUNM', 'SDHBG']
        for suffix in key_suffixes:
            key = os.getenv(f'FIRECRAWL_API_KEY_{suffix}')
            if key:
                keys.append(key)
        
        if not keys:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½• FIRECRAWL_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")
        
        print(f"âœ“ åŠ è½½äº† {len(keys)} ä¸ª API å¯†é’¥")
        return keys
    
    def get_next_key(self):
        """è·å–ä¸‹ä¸€ä¸ªå¯†é’¥ï¼ˆè½®è¯¢ï¼‰"""
        with self.lock:
            key = self.keys[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.keys)
            return key
    
    def get_key_count(self):
        """è·å–å¯†é’¥æ€»æ•°"""
        return len(self.keys)


def normalize_url(input_str):
    """å°†è¾“å…¥æ ‡å‡†åŒ–ä¸ºå®Œæ•´çš„ DeepWiki URL"""
    input_str = input_str.strip()
    
    if input_str.startswith('http'):
        return input_str
    
    # ç®€å†™å½¢å¼ï¼šawslabs/cli-agent-orchestrator
    if '/' in input_str and not input_str.startswith('/'):
        return f"{DEEPWIKI_BASE}/{input_str}"
    
    raise ValueError(f"æ— æ³•è¯†åˆ«çš„è¾“å…¥æ ¼å¼: {input_str}")


def scrape_page(url, key_manager, retry=3, base_delay=1.5, pbar=None):
    """ä½¿ç”¨ Firecrawl API æŠ“å–å•ä¸ªé¡µé¢ï¼Œæ”¯æŒå¤šå¯†é’¥è½®è¯¢å’Œé‡è¯•"""
    
    for attempt in range(retry):
        # è·å–ä¸‹ä¸€ä¸ªå¯†é’¥
        api_key = key_manager.get_next_key()
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'url': url,
            'formats': ['markdown', 'links']
        }
        
        try:
            response = requests.post(FIRECRAWL_API_URL, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # è¯·æ±‚æˆåŠŸåå»¶è¿Ÿï¼Œé¿å…é€Ÿç‡é™åˆ¶
            time.sleep(base_delay)
            
            if pbar:
                pbar.update(1)
            
            return {
                'url': url,
                'markdown': data.get('data', {}).get('markdown', ''),
                'links': data.get('data', {}).get('links', [])
            }
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                # é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
                wait_time = base_delay * (2 ** attempt)
                if pbar:
                    pbar.write(f"  âš  é€Ÿç‡é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
                
                if attempt == retry - 1:
                    raise Exception(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»ç„¶é‡åˆ°é€Ÿç‡é™åˆ¶: {url}")
            else:
                raise
                
        except Exception as e:
            if attempt == retry - 1:
                raise
            if pbar:
                pbar.write(f"  âš  é”™è¯¯: {e}ï¼Œé‡è¯•ä¸­...")
            time.sleep(base_delay)


def extract_project_links(main_page_data, project_path):
    """ä»ä¸»é¡µé¢æå–æ‰€æœ‰é¡¹ç›®ç›¸å…³çš„å­é¡µé¢é“¾æ¥"""
    links = main_page_data.get('links', [])
    project_links = []
    
    for link in links:
        if isinstance(link, str):
            url = link
        elif isinstance(link, dict):
            url = link.get('url', '')
        else:
            continue
        
        # åªä¿ç•™åŒé¡¹ç›®ä¸‹çš„å­é¡µé¢
        if url.startswith(f"{DEEPWIKI_BASE}/{project_path}/"):
            # æ’é™¤ä¸»é¡µé¢æœ¬èº«
            if url != f"{DEEPWIKI_BASE}/{project_path}":
                project_links.append(url)
    
    # å»é‡å¹¶æ’åº
    project_links = sorted(list(set(project_links)))
    return project_links


def scrape_pages_parallel(urls, key_manager, max_workers=2, base_delay=1.5):
    """å¹¶å‘æŠ“å–å¤šä¸ªé¡µé¢ï¼Œå¸¦è¿›åº¦æ¡"""
    results = []
    
    with tqdm(total=len(urls), desc="æŠ“å–å­é¡µé¢", unit="é¡µ") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {
                executor.submit(scrape_page, url, key_manager, base_delay=base_delay, pbar=pbar): url 
                for url in urls
            }
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    pbar.write(f"âœ— æŠ“å–å¤±è´¥ {url}: {e}")
    
    return results


def merge_markdown(main_page, sub_pages, project_name):
    """åˆå¹¶æ‰€æœ‰é¡µé¢çš„ Markdown å†…å®¹"""
    output = []
    
    # æ·»åŠ æ ‡é¢˜å’Œå…ƒä¿¡æ¯
    output.append(f"# {project_name}\n")
    output.append(f"*ç”Ÿæˆè‡ª DeepWiki*\n")
    output.append(f"*æºåœ°å€: {main_page['url']}*\n")
    output.append("\n---\n\n")
    
    # ä¸»é¡µé¢å†…å®¹
    output.append("## ä¸»é¡µé¢\n\n")
    output.append(main_page['markdown'])
    output.append("\n\n---\n\n")
    
    # å­é¡µé¢å†…å®¹
    for page in sub_pages:
        # ä» URL æå–é¡µé¢æ ‡é¢˜
        page_title = page['url'].split('/')[-1].replace('-', ' ').title()
        output.append(f"## {page_title}\n\n")
        output.append(page['markdown'])
        output.append("\n\n---\n\n")
    
    return ''.join(output)


def main():
    parser = argparse.ArgumentParser(description='DeepWiki æ–‡æ¡£æŠ“å–å·¥å…·ï¼ˆæ”¯æŒå¤šå¯†é’¥è½®è¯¢ï¼‰')
    parser.add_argument('url', help='DeepWiki URL æˆ–ç®€å†™å½¢å¼ (å¦‚: awslabs/cli-agent-orchestrator)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: é¡¹ç›®å.md)')
    parser.add_argument('-w', '--workers', type=int, default=2, help='å¹¶å‘æŠ“å–çº¿ç¨‹æ•° (é»˜è®¤: 2)')
    parser.add_argument('-d', '--delay', type=float, default=1.5, help='è¯·æ±‚é—´å»¶è¿Ÿç§’æ•° (é»˜è®¤: 1.5)')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–å¯†é’¥ç®¡ç†å™¨
        key_manager = APIKeyManager()
        
        # æ ‡å‡†åŒ– URL
        url = normalize_url(args.url)
        parsed = urlparse(url)
        project_path = parsed.path.strip('/')
        project_name = project_path.split('/')[-1]
        
        print(f"\nå¼€å§‹æŠ“å–é¡¹ç›®: {project_name}")
        print(f"URL: {url}")
        print(f"å¹¶å‘æ•°: {args.workers}, å»¶è¿Ÿ: {args.delay}s\n")
        
        # æŠ“å–ä¸»é¡µé¢
        print("\nğŸ“¥ æ­¥éª¤ 1/4: æŠ“å–ä¸»é¡µé¢...")
        main_page = scrape_page(url, key_manager, base_delay=args.delay)
        print("âœ“ ä¸»é¡µé¢æŠ“å–å®Œæˆ")
        
        # æå–å­é¡µé¢é“¾æ¥
        print("\nğŸ” æ­¥éª¤ 2/4: æå–å­é¡µé¢é“¾æ¥...")
        sub_links = extract_project_links(main_page, project_path)
        print(f"âœ“ å‘ç° {len(sub_links)} ä¸ªå­é¡µé¢")
        
        # å¹¶å‘æŠ“å–æ‰€æœ‰å­é¡µé¢
        print(f"\nğŸ“¦ æ­¥éª¤ 3/4: æŠ“å–æ‰€æœ‰å­é¡µé¢...")
        sub_pages = scrape_pages_parallel(sub_links, key_manager, max_workers=args.workers, base_delay=args.delay)
        print(f"âœ“ æˆåŠŸæŠ“å– {len(sub_pages)}/{len(sub_links)} ä¸ªå­é¡µé¢")
        
        # åˆå¹¶ Markdown
        print("\nğŸ“ æ­¥éª¤ 4/4: åˆå¹¶ Markdown...")
        merged_content = merge_markdown(main_page, sub_pages, project_name)
        
        # ä¿å­˜æ–‡ä»¶
        output_file = args.output or f"{project_name}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(merged_content)
        
        print(f"\n{'='*50}")
        print(f"âœ… å®Œæˆï¼")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š æ€»é¡µæ•°: {len(sub_pages) + 1}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(merged_content):,} å­—ç¬¦")
        print(f"{'='*50}\n")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
